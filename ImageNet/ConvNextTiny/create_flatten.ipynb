{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import convnext_tiny as model \n",
    "from torchvision.models import ConvNeXt_Tiny_Weights as model_weights\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo preentrenado de DenseNet121\n",
    "weights = model_weights.IMAGENET1K_V1\n",
    "model = model(weights=weights)\n",
    "model.eval()  # Poner el modelo en modo de evaluación\n",
    "\n",
    "# Definir las transformaciones para la imagen\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),  # Redimensionar la imagen a 256x256 píxeles\n",
    "    transforms.CenterCrop(224),  # Recortar la imagen al centro a 224x224 píxeles\n",
    "    transforms.ToTensor(),  # Convertir la imagen a un tensor de PyTorch\n",
    "    transforms.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),  # Normalizar la imagen\n",
    "])\n",
    "\n",
    "# Diccionario para mapear nombres de carpetas a labels personalizados\n",
    "custom_labels = {\n",
    "    'dolphin': 148,\n",
    "    'eagle': 22,\n",
    "    'falcon': 21,\n",
    "    'labrador': 208,\n",
    "    'lion': 291,\n",
    "    'persian': 283,\n",
    "    'shark': 2,\n",
    "    'tabby': 281,\n",
    "    'tiger': 292,\n",
    "    'wolf': 269,\n",
    "}\n",
    "\n",
    "# Función para transformar los targets/labels\n",
    "def custom_label_transform(target):\n",
    "    # Obtén el label original (nombre de la carpeta) del dataset\n",
    "    folder_name = dataset.classes[target]\n",
    "    # Retorna el label personalizado usando el diccionario\n",
    "    return custom_labels.get(folder_name, folder_name)  # Retorna el nombre de la carpeta si no se encuentra en el diccionario\n",
    "\n",
    "# Cargando el dataset con labels personalizados\n",
    "dataset = datasets.ImageFolder('../imagenet_data/', transform=preprocess, target_transform=custom_label_transform)\n",
    "\n",
    "\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, dataset, num_samples_per_class=2000):\n",
    "        self.dataset = dataset\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        \n",
    "        # Organizar los datos por clase\n",
    "        self.indices_per_class = {}\n",
    "        for idx, (_, label) in enumerate(self.dataset):\n",
    "            if label not in self.indices_per_class:\n",
    "                self.indices_per_class[label] = []\n",
    "            self.indices_per_class[label].append(idx)\n",
    "        \n",
    "        # Seleccionar aleatoriamente num_samples_per_class índices para cada clase\n",
    "        self.balanced_indices = []\n",
    "        for label, indices in self.indices_per_class.items():\n",
    "            if len(indices) >= num_samples_per_class:\n",
    "                self.balanced_indices.extend(random.sample(indices, num_samples_per_class))\n",
    "            else:\n",
    "                # Si hay menos de num_samples_per_class, reutiliza algunos índices\n",
    "                self.balanced_indices.extend(indices * (num_samples_per_class // len(indices)) + random.sample(indices, num_samples_per_class % len(indices)))\n",
    "        \n",
    "        # Mezclar los índices para asegurar aleatoriedad en el DataLoader\n",
    "        random.shuffle(self.balanced_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Obtener el índice original del conjunto de datos\n",
    "        original_index = self.balanced_indices[index]\n",
    "        return self.dataset[original_index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.balanced_indices)\n",
    "\n",
    "# Suponiendo que `original_dataset` es tu conjunto de datos original\n",
    "balanced_dataset = BalancedDataset(dataset)\n",
    "\n",
    "# Crear un DataLoader para cargar los datos en lotes\n",
    "dataloader = torch.utils.data.DataLoader(balanced_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images, labels = next(iter(dataloader))\n",
    "#print(\"Pred:\", model(images).max(1).indices)\n",
    "#print(\"Real:\", labels)\n",
    "##imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x30aee7280>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook(module, input, output):\n",
    "    tensors.append(output.clone().detach())\n",
    "model.classifier[1].register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 313/313 [24:59<00:00,  4.79s/it]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "used_images = []\n",
    "tensors = []\n",
    "\n",
    "for images, labels in tqdm(dataloader, desc='Processing'):\n",
    "    used_images.append(images)\n",
    "    y.append(labels)\n",
    "    model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_images = torch.cat(used_images, dim=0)\n",
    "y = torch.cat(y, dim=0).tolist()\n",
    "tensors = torch.cat(tensors, dim=0)\n",
    "\n",
    "torch.save(used_images, \"images.pth\")\n",
    "torch.save(tensors, \"tensors.pth\")\n",
    "torch.save(y, \"labels.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nobsp_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
