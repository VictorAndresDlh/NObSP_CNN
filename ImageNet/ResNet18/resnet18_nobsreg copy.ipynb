{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import config\n",
    "import torch\n",
    "from app.Classifiers.Classifier_NN import Classifier_IMGNET, Classifier_MNIST\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from app.NObSP.NObSP_Decomposition import NObSP_NN_single_MultiOutput, NObSP_NN_single_MultiOutput_reg\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader as tf_dataloader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from matplotlib import colormaps\n",
    "import PIL\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los tensores guardados\n",
    "X = torch.load('tensors.pth')\n",
    "X = torch.stack(X) # Convierte 'X' en un tensor si es una lista de listas\n",
    "X_small = torch.load('tensors_avgpooling.pth')\n",
    "y = torch.load('labels.pth')\n",
    "y = torch.tensor(y)  # Convierte 'y' en un tensor si es una lista de enteros\n",
    "y = torch.nn.functional.one_hot(y, num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = []\n",
    "X_small_sample = []\n",
    "y_sample = []\n",
    "index_used = []\n",
    "\n",
    "sample_size = 200\n",
    "y_values = [148, 22, 21, 208, 291, 283, 2, 281, 292, 269]\n",
    "\n",
    "for i in y_values:\n",
    "    indices = torch.where(y.argmax(dim=1) == i)[0]\n",
    "    indices = indices[torch.randperm(len(indices))[:sample_size]]\n",
    "    index_used.append(indices)\n",
    "    X_sample.append(X[indices])\n",
    "    X_small_sample.append(X_small[indices])\n",
    "    y_sample.append(y[indices])\n",
    "\n",
    "# Verifica que cada tensor en X_sample y y_sample es diferente\n",
    "for i in range(len(X_sample)):\n",
    "    for j in range(i + 1, len(X_sample)):\n",
    "        assert id(X_sample[i]) != id(X_sample[j])\n",
    "        assert id(y_sample[i]) != id(y_sample[j])\n",
    "\n",
    "index_used = [tensor.item() for sublist in index_used for tensor in sublist]\n",
    "X_sample = torch.cat(X_sample)\n",
    "y_sample = torch.cat(y_sample)\n",
    "X_small_sample = torch.cat(X_small_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18 as model \n",
    "from torchvision.models import ResNet18_Weights as model_weights\n",
    "# Cargar el modelo preentrenado de DenseNet121\n",
    "weights = model_weights.IMAGENET1K_V1\n",
    "model = model(weights=weights)\n",
    "model.eval()  # Poner el modelo en modo de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 25088])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierBig(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ClassifierBig, self).__init__()\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.weights = model.fc.weight.data\n",
    "        self.classifier = nn.Linear(512, 1000)\n",
    "        self.classifier.weight.data = self.weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 512, 7, 7)\n",
    "        x = self.avgpool(x)\n",
    "        x_t = x.view(x.size(0), -1)  \n",
    "        y_lin = self.classifier(x_t)\n",
    "        prob = F.softmax(y_lin, dim=1)\n",
    "        return prob, x_t, y_lin\n",
    "\n",
    "classifier_big = ClassifierBig(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierSmall(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ClassifierSmall, self).__init__()\n",
    "        self.weights = model.fc.weight.data\n",
    "        self.classifier = nn.Linear(512, 1000)\n",
    "        self.classifier.weight.data = self.weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_lin = self.classifier(x)\n",
    "        prob = F.softmax(x, dim=1)\n",
    "        return prob, x, y_lin\n",
    "\n",
    "classifier_small = ClassifierSmall(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_big.eval()  # Setting the model in evaluation mode\n",
    "with torch.inference_mode():\n",
    "    y_prob_1, x_trans_total_1, y_est_1 = classifier_big(\n",
    "        X_sample\n",
    "    )  # Computing the nonlinear transformation of the input data X\n",
    "    \n",
    "classifier_small.eval()  # Setting the model in evaluation mode\n",
    "with torch.inference_mode():\n",
    "    y_prob_2, x_trans_total_2, y_est_2 = classifier_small(\n",
    "        X_small_sample\n",
    "    )  # Computing the nonlinear transformation of the input data X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(np.argmax(F.softmax(y_est_1, dim=1), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  torch.Size([2000, 512])\n",
      "beta:  torch.Size([512, 512, 1000])\n",
      "[  2   3   4   5   6  10  13  15  16  17  18  19  20  21  22  23  24  33\n",
      "  36  48  49  65  73  81  83  85  86  87  89  91  92  93  95  98  99 103\n",
      " 104 105 106 107 111 123 126 127 129 133 135 137 138 140 143 144 146 147\n",
      " 148 149 150 154 155 159 160 162 163 164 165 167 169 170 172 173 174 175\n",
      " 176 177 178 179 182 191 205 206 207 208 209 210 211 212 215 218 220 222\n",
      " 223 224 225 227 234 235 236 237 243 244 245 249 256 257 259 260 261 262\n",
      " 263 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284\n",
      " 285 286 287 289 291 292 294 295 296 299 306 307 330 331 332 333 336 339\n",
      " 341 342 343 344 347 350 351 352 355 358 359 366 369 371 373 385 388 390\n",
      " 391 394 397 408 410 430 437 450 478 517 559 562 588 704 733 750 760 761\n",
      " 770 801 814 831 833 842 851 858 863 871 897 900 913 917 920 971 976 977\n",
      " 978 980 983 993 996]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd737e87379544759ec9b7339d1e50fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Number of tensors:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if os.path.exists(\"betas_tensor_small.pth\"):\n",
    "    betas_tensor_small = torch.load(\"betas_tensor_small.pth\")\n",
    "else:\n",
    "    betas_tensor_small = NObSP_NN_single_MultiOutput_reg(\n",
    "        X_small_sample, y_est_2, classifier_small\n",
    "    )  # Computing the decomposition using NObSP. The Alpha parameters are the weigths for the Interpretation Layer\n",
    "    torch.save(betas_tensor_small, \"betas_tensor_small.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  torch.Size([2000, 25088])\n",
      "beta:  torch.Size([512, 25088, 1000])\n",
      "[  2   3   4   5   6  10  13  15  16  17  18  19  20  21  22  23  24  33\n",
      "  36  48  49  65  73  81  82  83  85  86  87  89  91  93  95  98  99 103\n",
      " 104 105 106 107 111 123 126 127 129 133 135 137 138 140 143 144 146 147\n",
      " 148 149 150 154 155 159 160 162 163 164 165 167 169 170 172 173 174 175\n",
      " 176 177 178 179 182 191 205 206 207 208 209 210 211 212 215 218 220 222\n",
      " 223 224 225 227 234 235 236 237 243 244 245 249 256 257 259 260 261 262\n",
      " 263 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284\n",
      " 285 286 287 289 291 292 293 294 295 296 299 306 307 330 331 332 333 336\n",
      " 339 341 342 343 344 347 350 351 352 355 358 359 366 369 371 373 385 388\n",
      " 390 391 394 397 408 410 430 437 450 478 517 559 562 588 704 733 750 760\n",
      " 761 770 801 814 831 833 842 851 858 863 871 897 900 913 917 920 971 976\n",
      " 977 978 980 983 993 996]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a21825778a7447491f8e3cc90523b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Number of tensors:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     betas_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas_tensor.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     betas_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mNObSP_NN_single_MultiOutput_reg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_est_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_big\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Computing the decomposition using NObSP. The Alpha parameters are the weigths for the Interpretation Layer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(betas_tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas_tensor.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NObSP_CNN/ImageNet/ResNet18/../../app/NObSP/NObSP_Decomposition.py:168\u001b[0m, in \u001b[0;36mNObSP_NN_single_MultiOutput_reg\u001b[0;34m(X, y_est, model)\u001b[0m\n\u001b[1;32m    165\u001b[0m     X_target_sub\u001b[38;5;241m.\u001b[39msub_(torch\u001b[38;5;241m.\u001b[39mmean(X_target_sub,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    166\u001b[0m     X_reference_sub\u001b[38;5;241m.\u001b[39msub_(torch\u001b[38;5;241m.\u001b[39mmean(X_reference_sub,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m--> 168\u001b[0m     beta \u001b[38;5;241m=\u001b[39m \u001b[43mobsp_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_target_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_reference_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_est\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_est\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     beta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(neurons_last)\n",
      "File \u001b[0;32m~/NObSP_CNN/ImageNet/ResNet18/../../app/NObSP/NObSP_Decomposition.py:51\u001b[0m, in \u001b[0;36mobsp_regression\u001b[0;34m(X, Z, y)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobsp_regression\u001b[39m(X, Z, y):\n\u001b[1;32m     38\u001b[0m     \n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Oblique projection using regression approach.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# Compute the matrix Q\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     P \u001b[38;5;241m=\u001b[39m Z\u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;129;43m@Z\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mt(Z) \u001b[38;5;66;03m# Computing the orthogonal projection matriz onto the subsapce given by Y    \u001b[39;00m\n\u001b[1;32m     52\u001b[0m     Q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(Z\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m-\u001b[39mP \u001b[38;5;66;03m# Computing the complement of P\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Compute the matrix (X^T Q X)^{-1} X^T Q\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"betas_tensor.pth\"):\n",
    "    betas_tensor = torch.load(\"betas_tensor.pth\")\n",
    "else:\n",
    "    betas_tensor = NObSP_NN_single_MultiOutput_reg(\n",
    "        X_sample, y_est_1, classifier_big\n",
    "    )  # Computing the decomposition using NObSP. The Alpha parameters are the weigths for the Interpretation Layer\n",
    "    torch.save(betas_tensor, \"betas_tensor.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contrib(X, model, betas):\n",
    "    X_new = X.clone()\n",
    "    clase = int(torch.argmax(model(X_new.unsqueeze(0))[0].detach(), axis=1))\n",
    "    X_target_tot = torch.zeros((1, list(model.children())[-1].in_features))\n",
    "\n",
    "    X_temp = X_new.repeat(len(X_new), 1)\n",
    "    X_temp = torch.eye(len(X_new)) * X_temp\n",
    "    with torch.inference_mode():\n",
    "        _, X_latent, _ = model(X_new.unsqueeze(0))\n",
    "        _, X_target, _ = model(X_temp)\n",
    "    X_target_tot = torch.cat((X_target_tot, X_target), 0)\n",
    "    X_target_tot = X_target_tot[1:]\n",
    "\n",
    "    contrib = (X_target_tot @ betas[:, :, clase]).diag()\n",
    "    return clase, contrib, X_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contribs(X, model, betas):\n",
    "    contribs = []\n",
    "    labels = []\n",
    "    tensors = []\n",
    "    latents = []\n",
    "    for i in tqdm(range(len(X))):\n",
    "        label, contrib, latent = get_contrib(X[i], model, betas)\n",
    "        contribs.append(contrib.numpy())\n",
    "        labels.append(label)\n",
    "        tensors.append(X[i].numpy())\n",
    "        latents.append(latent.numpy()[0])\n",
    "\n",
    "    train_contribs = pd.DataFrame(\n",
    "        {\n",
    "            'Contrib': contribs,\n",
    "            'Label': labels,\n",
    "            'Flatten': tensors,\n",
    "            'Latent': latents\n",
    "        }\n",
    "    )\n",
    "    return train_contribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"train_contribs.pkl\"):\n",
    "    train_contribs = pd.read_pickle(\"train_contribs.pkl\")\n",
    "else: \n",
    "    train_contribs = train_contribs(X_sample, classifier_big, betas_tensor)\n",
    "    train_contribs.to_pickle(\"train_contribs.pkl\")\n",
    "    \n",
    "train_contribs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_contrib(train_contribs):\n",
    "    \n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        random_state=42,\n",
    "        perplexity=20,\n",
    "    )\n",
    "    tsne_contrib = tsne.fit_transform(train_contribs['Contrib'].apply(pd.Series))\n",
    "    tsne_contrib = pd.DataFrame(tsne_contrib, columns=['tsne_1', 'tsne_2'])\n",
    "    \n",
    "    tsne_flatten = tsne.fit_transform(train_contribs['Flatten'].apply(pd.Series))\n",
    "    tsne_flatten = pd.DataFrame(tsne_flatten, columns=['tsne_1', 'tsne_2'])\n",
    "    \n",
    "    tsne_latent = tsne.fit_transform(train_contribs['Latent'].apply(pd.Series))\n",
    "    tsne_latent = pd.DataFrame(tsne_latent, columns=['tsne_1', 'tsne_2'])\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 18))\n",
    "    sns.scatterplot(\n",
    "        data=tsne_contrib,\n",
    "        x='tsne_1', y='tsne_2',\n",
    "        hue=train_contribs['Label'],\n",
    "        palette='Paired',\n",
    "        ax=ax[0]\n",
    "    )\n",
    "    ax[0].set_title('Contributions')\n",
    "    ax[0].grid(alpha=0.4)\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=tsne_flatten,\n",
    "        x='tsne_1', y='tsne_2',\n",
    "        hue=train_contribs['Label'],\n",
    "        palette='Paired',\n",
    "        ax=ax[1]\n",
    "    )\n",
    "    ax[1].set_title('Flatten')\n",
    "    ax[1].grid(alpha=0.4)\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        data=tsne_latent,\n",
    "        x='tsne_1', y='tsne_2',\n",
    "        hue=train_contribs['Label'],\n",
    "        palette='Paired',\n",
    "        ax=ax[2]\n",
    "    )\n",
    "    ax[2].set_title('Latent')\n",
    "    ax[2].grid(alpha=0.4)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_contrib(train_contribs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contribs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18 as model \n",
    "from torchvision.models import ResNet18_Weights as model_weights\n",
    "# Cargar el modelo preentrenado de DenseNet121\n",
    "weights = model_weights.IMAGENET1K_V1\n",
    "model = model(weights=weights)\n",
    "model.eval()  # Poner el modelo en modo de evaluación\n",
    "\n",
    "\n",
    "def hook(module, input, output):\n",
    "    global flatten\n",
    "    flatten.append(output.clone().detach())\n",
    "    flatten = [tensor.view(-1) for batch in flatten for tensor in batch]\n",
    "    \n",
    "model.layer4[-1].bn2.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top5_cosine(data, y, index, flatten, contrib, latent, net_label, df): #incluir label original, label del modelo\n",
    "    \n",
    "    # Convertir cada array en la columna del DataFrame en un tensor\n",
    "    contrib_list = [torch.tensor(x) for x in df['Contrib']]\n",
    "    flatten_list = [torch.tensor(x) for x in df['Flatten']]\n",
    "    latent_list = [torch.tensor(x) for x in df['Latent']]\n",
    "    \n",
    "\n",
    "    # Calcular la similitud del coseno para las contribuciones\n",
    "    similarities_contrib = [F.cosine_similarity(contrib, tensor, dim=0) for tensor in contrib_list]\n",
    "    similarities_contrib = torch.tensor(similarities_contrib)\n",
    "    \n",
    "    # Calcular la similitud del coseno para los flatten\n",
    "    similarities_flatten = [F.cosine_similarity(flatten, tensor, dim=0) for tensor in flatten_list]\n",
    "    similarities_flatten = torch.tensor(similarities_flatten)\n",
    "    \n",
    "   # Calcular la similitud del coseno para los flatten\n",
    "    similarities_latent = [F.cosine_similarity(latent, tensor, dim=0) for tensor in latent_list]\n",
    "    similarities_latent = torch.tensor(similarities_latent) \n",
    "\n",
    "    # Encontrar los top 5\n",
    "    top5_indices_contrib = torch.topk(similarities_contrib, 5).indices\n",
    "    top5_indices_flatten = torch.topk(similarities_flatten, 5).indices\n",
    "    top5_indices_latent = torch.topk(similarities_latent, 5).indices\n",
    "\n",
    "    # Visualizar la imagen original\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(15, 10))\n",
    "    img = torchvision.utils.make_grid(data[index]) / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    axs[0, 2].imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    axs[0, 2].set_title(f\"Real Label: {torch.argmax(y[index])}\\n Model Label: {net_label}\")\n",
    "\n",
    "    # Desactivar los espacios vacíos en la primera fila\n",
    "    for i in [0, 1, 3, 4]:\n",
    "        axs[0, i].axis(\"off\")\n",
    "\n",
    "    # Visualizar las imágenes más similares para los tensores\n",
    "    for i, idx in enumerate(top5_indices_contrib):\n",
    "        img = torchvision.utils.make_grid(images_sample[idx]) / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        axs[1, i].imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        axs[1, i].set_title(f\"Contrib {i+1} - Sim: {similarities_contrib[idx]:.3f}\\n Real Label: {torch.argmax(y_sample[idx])}\")\n",
    "\n",
    "    # Visualizar las imágenes más similares para los flatten\n",
    "    for i, idx in enumerate(top5_indices_flatten):\n",
    "        img = torchvision.utils.make_grid(images_sample[idx]) / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        axs[2, i].imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        axs[2, i].set_title(f\"Flatten {i+1} - Sim: {similarities_flatten[idx]:.3f}\\n Real Label: {torch.argmax(y_sample[idx])}\")\n",
    "\n",
    "    # Visualizar las imágenes más similares para los flatten\n",
    "    for i, idx in enumerate(top5_indices_latent):\n",
    "        img = torchvision.utils.make_grid(images_sample[idx]) / 2 + 0.5\n",
    "        npimg = img.numpy()\n",
    "        axs[3, i].imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "        axs[3, i].set_title(f\"Latent {i+1} - Sim: {similarities_latent[idx]:.3f}\\n Real Label: {torch.argmax(y_sample[idx])}\")\n",
    "    \n",
    "    for ax in axs.flat:\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_lista(lista, top_n, positivo=True):\n",
    "    if positivo:\n",
    "        elementos = [(i, num) for i, num in enumerate(lista) if num > 0]\n",
    "    else:\n",
    "        elementos = [(i, num) for i, num in enumerate(lista) if num < 0]\n",
    "    total_elementos = sum(num for _, num in elementos)\n",
    "    porcentajes = [(i, num, num / total_elementos * 100) for i, num in elementos]\n",
    "    porcentajes_ordenados = sorted(porcentajes, key=lambda x: x[2], reverse=positivo)\n",
    "    return porcentajes_ordenados[:top_n]\n",
    "\n",
    "def graficar_elementos(ax, resultado, top_n, positivo=True):\n",
    "    indices = [tupla[0] for tupla in resultado]\n",
    "    entradas = [tupla[1] for tupla in resultado]\n",
    "    porcentajes = [tupla[2] for tupla in resultado]\n",
    "\n",
    "    color = 'tab:blue' if positivo else 'tab:red'\n",
    "    ax.barh(range(len(indices)), porcentajes, color = color)\n",
    "\n",
    "    etiquetas_y = [f\"Entrada: {i}, Valor: {e:.3f}\" for i, e in zip(indices, entradas)]\n",
    "    ax.set_yticks(range(len(indices)))\n",
    "    ax.set_yticklabels(etiquetas_y)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.set_xlabel('Porcentaje (%)')\n",
    "    if positivo:\n",
    "        ax.set_title(f'Top {top_n} Entradas Positivas por Porcentaje')\n",
    "    else:\n",
    "        ax.set_title(f'Top {top_n} Entradas Negativas por Porcentaje')\n",
    "\n",
    "def graficar_entradas(tensor, top_n=15):\n",
    "    lista = tensor.tolist()\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "    resultado_positivos = procesar_lista(lista, top_n, positivo=True)\n",
    "    graficar_elementos(axs[0], resultado_positivos, top_n, positivo=True)\n",
    "\n",
    "    resultado_negativos = procesar_lista(lista, top_n, positivo=False)\n",
    "    graficar_elementos(axs[1], resultado_negativos, top_n, positivo=False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = torch.load('images.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 250\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "ax.imshow(images[index].permute(1, 2, 0))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Real Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = []\n",
    "with torch.inference_mode():\n",
    "    net_label = model(images[index].unsqueeze(0))\n",
    "    net_label = torch.argmax(net_label)\n",
    "    _, contrib, latent = get_contrib(flatten[0], classifier_big, betas_tensor)\n",
    "    avgpool = torch.nn.AdaptiveAvgPool2d((1,1))(flatten[0].view(512, 7, 7)).view(-1)\n",
    "    _, avg_contrib, _ = get_contrib(avgpool, classifier_small, betas_tensor_small)\n",
    "\n",
    "\n",
    "top5_cosine(images, y, index, flatten[0], contrib, latent[0], net_label, train_contribs)\n",
    "graficar_entradas(contrib, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten2 = flatten[0].clone()\n",
    "contrib2 = contrib.clone()\n",
    "shape = (7, 512, 7)\n",
    "\n",
    "nobs_cam = flatten2*contrib2\n",
    "nobs_cam = nobs_cam.view(shape)\n",
    "\n",
    "nobs_cam_mean = torch.mean(nobs_cam, dim=1)\n",
    "nobs_cam_mean = (nobs_cam_mean - nobs_cam_mean.min()) / (nobs_cam_mean.max() - nobs_cam_mean.min()) # hacer una prueba con el softmax\n",
    "\n",
    "nobs_cam_avg = torch.matmul(nobs_cam.permute(0, 2, 1), avg_contrib) / torch.sum(avg_contrib)\n",
    "nobs_cam_avg = (nobs_cam_avg - nobs_cam_avg.min()) / (nobs_cam_avg.max() - nobs_cam_avg.min())\n",
    "\n",
    "contrib2 = contrib2.view(shape)\n",
    "contrib2 = torch.mean(contrib2, dim=1)\n",
    "contrib2 = (contrib2 - contrib2.min()) / (contrib2.max() - contrib2.min())\n",
    "\n",
    "flatten2 = flatten2.view(shape)\n",
    "flatten2 = torch.mean(flatten2, dim=1)\n",
    "flatten2 = (flatten2 - flatten2.min()) / (flatten2.max() - flatten2.min())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "ax.imshow(images[index].permute(1, 2, 0))\n",
    "ax.axis('off')\n",
    "ax.set_title(\"Real Image\")\n",
    "plt.show()\n",
    "\n",
    "cmap = 'viridis'\n",
    "fig, ax = plt.subplots(1, 4, figsize=(12, 3))  # Aumentamos el ancho de la figura\n",
    "ax[0].imshow(flatten2.squeeze(), cmap=cmap)\n",
    "ax[0].set_title(\"Flatten\")\n",
    "cbar0 = plt.colorbar(ax[0].imshow(flatten2.squeeze(), cmap=cmap), ax=ax[0])\n",
    "\n",
    "ax[1].imshow(contrib2.squeeze(), cmap=cmap)\n",
    "ax[1].set_title(\"Contrib\")\n",
    "cbar1 = plt.colorbar(ax[1].imshow(contrib2.squeeze(), cmap=cmap), ax=ax[1])\n",
    "\n",
    "ax[2].imshow(nobs_cam_mean.squeeze(), cmap=cmap)\n",
    "ax[2].set_title(\"NObSP CAM\")\n",
    "cbar2 = plt.colorbar(ax[2].imshow(nobs_cam_mean.squeeze(), cmap=cmap), ax=ax[2])\n",
    "\n",
    "ax[3].imshow(nobs_cam_avg.squeeze(), cmap=cmap)\n",
    "ax[3].set_title(\"NObSP CAM - Weight AVG\")\n",
    "cbar2 = plt.colorbar(ax[3].imshow(nobs_cam_avg.squeeze(), cmap=cmap), ax=ax[3])\n",
    "\n",
    "for ax in ax.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()  # Ajusta el espaciado entre las subtramas\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "pad = (1,)*2\n",
    "# no rellenar con ceros\n",
    "\n",
    "nobs_cam_pad = np.pad(nobs_cam_mean, pad_width=(pad, pad), mode='constant', constant_values=0)# no rellenar con ceros \n",
    "overlay_mean = to_pil_image(nobs_cam_pad, mode='F').resize((224,224), resample=PIL.Image.BICUBIC)\n",
    "cmap = colormaps['jet']\n",
    "overlay_mean = (cmap(np.asarray(overlay_mean) ** 2)[:, :, :3])\n",
    "\n",
    "input_tensor = images[index].unsqueeze(0)\n",
    "input_numpy = input_tensor.detach().cpu().numpy()\n",
    "input_numpy = input_numpy - input_numpy.min()\n",
    "input_numpy = input_numpy / input_numpy.max()\n",
    "input_numpy = np.transpose(input_numpy.squeeze(), (1, 2, 0))\n",
    "\n",
    "\n",
    "ax.imshow(input_numpy)\n",
    "ax.imshow(overlay_mean, alpha=0.5)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "pad = (1,)*2\n",
    "# no rellenar con ceros\n",
    "\n",
    "nobs_cam_pad = np.pad(nobs_cam_avg, pad_width=(pad, pad), mode='constant', constant_values=0)# no rellenar con ceros \n",
    "overlay_avg = to_pil_image(nobs_cam_pad, mode='F').resize((224,224), resample=PIL.Image.BICUBIC)\n",
    "cmap = colormaps['jet']\n",
    "overlay_avg = (cmap(np.asarray(overlay_avg) ** 2)[:, :, :3])\n",
    "\n",
    "input_tensor = images[index].unsqueeze(0)\n",
    "input_numpy = input_tensor.detach().cpu().numpy()\n",
    "input_numpy = input_numpy - input_numpy.min()\n",
    "input_numpy = input_numpy / input_numpy.max()\n",
    "input_numpy = np.transpose(input_numpy.squeeze(), (1, 2, 0))\n",
    "\n",
    "\n",
    "ax.imshow(input_numpy)\n",
    "ax.imshow(overlay_avg, alpha=0.5)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [model.layer4[-1]]\n",
    "input_tensor = images[index].unsqueeze(0)\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "targets = [ClassifierOutputTarget(torch.argmax(y[index]))]\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "\n",
    "# Normalizar el tensor a [0, 1] y convertirlo a un array de NumPy\n",
    "input_numpy = input_tensor.detach().cpu().numpy()\n",
    "input_numpy = input_numpy - input_numpy.min()\n",
    "input_numpy = input_numpy / input_numpy.max()\n",
    "\n",
    "# Ajustar la forma de la máscara para que coincida con la de la imagen\n",
    "grayscale_cam = grayscale_cam.squeeze()\n",
    "\n",
    "# Transponer input_numpy para que tenga la forma (3,224,224)\n",
    "input_numpy = np.transpose(input_numpy.squeeze(), (1, 2, 0))\n",
    "\n",
    "cam_image = show_cam_on_image(input_numpy, grayscale_cam, use_rgb=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "#ax.imshow(images[index].permute(1, 2, 0))\n",
    "ax.imshow(cam_image)\n",
    "ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "ax[0].imshow(input_numpy)\n",
    "ax[0].imshow(overlay_mean, alpha=0.5)\n",
    "ax[0].set_title(\"NObSP CAM\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(input_numpy)\n",
    "ax[1].imshow(overlay_avg, alpha=0.5)\n",
    "ax[1].set_title(\"NObSP CAM - Weight AVG Contrib\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(cam_image)\n",
    "ax[2].axis('off')\n",
    "ax[2].set_title(\"GradCAM\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
