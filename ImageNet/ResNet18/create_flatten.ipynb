{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18 as model \n",
    "from torchvision.models import ResNet18_Weights as model_weights\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo preentrenado de DenseNet121\n",
    "weights = model_weights.IMAGENET1K_V1\n",
    "model = model(weights=weights)\n",
    "model.eval()  # Poner el modelo en modo de evaluación\n",
    "\n",
    "# Definir las transformaciones para la imagen\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),  # Redimensionar la imagen a 256x256 píxeles\n",
    "    transforms.CenterCrop(224),  # Recortar la imagen al centro a 224x224 píxeles\n",
    "    transforms.ToTensor(),  # Convertir la imagen a un tensor de PyTorch\n",
    "    transforms.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),  # Normalizar la imagen\n",
    "])\n",
    "\n",
    "# Diccionario para mapear nombres de carpetas a labels personalizados\n",
    "custom_labels = {\n",
    "    'dolphin': 148,\n",
    "    'eagle': 22,\n",
    "    'falcon': 21,\n",
    "    'labrador': 208,\n",
    "    'lion': 291,\n",
    "    'persian': 283,\n",
    "    'shark': 2,\n",
    "    'tabby': 281,\n",
    "    'tiger': 292,\n",
    "    'wolf': 269,\n",
    "}\n",
    "\n",
    "# Función para transformar los targets/labels\n",
    "def custom_label_transform(target):\n",
    "    # Obtén el label original (nombre de la carpeta) del dataset\n",
    "    folder_name = dataset.classes[target]\n",
    "    # Retorna el label personalizado usando el diccionario\n",
    "    return custom_labels.get(folder_name, folder_name)  # Retorna el nombre de la carpeta si no se encuentra en el diccionario\n",
    "\n",
    "# Cargando el dataset con labels personalizados\n",
    "dataset = datasets.ImageFolder('../imagenet_data/', transform=preprocess, target_transform=custom_label_transform)\n",
    "\n",
    "\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, dataset, num_samples_per_class=2000):\n",
    "        self.dataset = dataset\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        \n",
    "        # Organizar los datos por clase\n",
    "        self.indices_per_class = {}\n",
    "        for idx, (_, label) in enumerate(self.dataset):\n",
    "            if label not in self.indices_per_class:\n",
    "                self.indices_per_class[label] = []\n",
    "            self.indices_per_class[label].append(idx)\n",
    "        \n",
    "        # Seleccionar aleatoriamente num_samples_per_class índices para cada clase\n",
    "        self.balanced_indices = []\n",
    "        for label, indices in self.indices_per_class.items():\n",
    "            if len(indices) >= num_samples_per_class:\n",
    "                self.balanced_indices.extend(random.sample(indices, num_samples_per_class))\n",
    "            else:\n",
    "                # Si hay menos de num_samples_per_class, reutiliza algunos índices\n",
    "                self.balanced_indices.extend(indices * (num_samples_per_class // len(indices)) + random.sample(indices, num_samples_per_class % len(indices)))\n",
    "        \n",
    "        # Mezclar los índices para asegurar aleatoriedad en el DataLoader\n",
    "        random.shuffle(self.balanced_indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Obtener el índice original del conjunto de datos\n",
    "        original_index = self.balanced_indices[index]\n",
    "        return self.dataset[original_index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.balanced_indices)\n",
    "\n",
    "# Suponiendo que `original_dataset` es tu conjunto de datos original\n",
    "balanced_dataset = BalancedDataset(dataset)\n",
    "\n",
    "# Crear un DataLoader para cargar los datos en lotes\n",
    "dataloader = torch.utils.data.DataLoader(balanced_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images, labels = next(iter(dataloader))\n",
    "#print(\"Pred:\", model(images).max(1).indices)\n",
    "#print(\"Real:\", labels)\n",
    "##imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x1660d4c40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook(module, input, output):\n",
    "    tensors.append(output.clone().detach())\n",
    "model.layer4[-1].bn2.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 313/313 [10:39<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "used_images = []\n",
    "tensors = []\n",
    "\n",
    "for images, labels in tqdm(dataloader, desc='Processing'):\n",
    "    used_images.append(images)\n",
    "    y.append(labels)\n",
    "    model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "used_images = torch.cat(used_images, dim=0)\n",
    "y = torch.cat(y, dim=0).tolist()\n",
    "tensors = [tensor.view(-1) for batch in tensors for tensor in batch]\n",
    "\n",
    "torch.save(used_images, \"images.pth\")\n",
    "torch.save(tensors, \"tensors.pth\")\n",
    "torch.save(y, \"labels.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nobsp_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
