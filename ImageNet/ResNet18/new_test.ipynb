{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import config\n",
    "import torch\n",
    "from app.Classifiers.Classifier_NN import Classifier_IMGNET, Classifier_MNIST\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from app.NObSP.NObSP_Decomposition import NObSP_NN_single_MultiOutput, NObSP_NN_single_MultiOutput_reg\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader as tf_dataloader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from matplotlib import colormaps\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los tensores guardados\n",
    "images = torch.load('images.pth')\n",
    "X = torch.load('tensors_avgpooling.pth') # Convierte 'X' en un tensor si es una lista de listas\n",
    "y = torch.load('labels.pth')\n",
    "\n",
    "y = torch.tensor(y)  # Convierte 'y' en un tensor si es una lista de enteros\n",
    "\n",
    "\n",
    "# Suponiendo que 'tensor_labels' es tu tensor de etiquetas\n",
    "tensor_labels = y\n",
    "\n",
    "# Paso 1: Extraer las etiquetas únicas y crear el mapeo\n",
    "unique_labels = torch.unique(tensor_labels).tolist()\n",
    "label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "\n",
    "# Paso 2: Convertir etiquetas a índices mapeados\n",
    "mapped_indices = torch.tensor([label_to_index[label.item()] for label in tensor_labels])\n",
    "\n",
    "# Paso 3: Aplicar one-hot encoding\n",
    "y = torch.nn.functional.one_hot(mapped_indices, num_classes=len(unique_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample = []\n",
    "y_sample = []\n",
    "index_used = []\n",
    "\n",
    "sample_size = 400\n",
    "# Supongamos que 'num_classes' es el número de clases en tus etiquetas\n",
    "for i in range(10):\n",
    "    indices = torch.where(y.argmax(dim=1) == i)[0]\n",
    "    indices = indices[torch.randperm(len(indices))[:sample_size]]\n",
    "    index_used.append(indices)\n",
    "    X_sample.append(X[indices])\n",
    "    y_sample.append(y[indices])\n",
    "\n",
    "# Verifica que cada tensor en X_sample y y_sample es diferente\n",
    "for i in range(len(X_sample)):\n",
    "    for j in range(i + 1, len(X_sample)):\n",
    "        assert id(X_sample[i]) != id(X_sample[j])\n",
    "        assert id(y_sample[i]) != id(y_sample[j])\n",
    "\n",
    "index_used = [tensor.item() for sublist in index_used for tensor in sublist]\n",
    "images_sample = [images[i] for i in index_used]\n",
    "X_sample = torch.cat(X_sample)\n",
    "y_sample = torch.cat(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.size(X_sample, 0); # Defining the number of datapoints\n",
    "in_feat = np.size(X_sample, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, N)\n",
    "train_split = int(0.9 * N)\n",
    "p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split of the data for training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sample,\n",
    "    y_sample,\n",
    "    train_size = train_split,\n",
    "    random_state=42\n",
    ") \n",
    "\n",
    "## Creating model for single nonlinear effects\n",
    "\n",
    "classifier = Classifier_MNIST(in_number=X_sample.shape[1], out_number=y_sample.shape[1]) # Creating the model\n",
    "loss_fcn = nn.CrossEntropyLoss() # Definning loss function\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop for the model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 2.30261 | test Loss: 2.30192\n",
      "Epoch: 10 | Loss: 2.28748 | test Loss: 2.28615\n",
      "Epoch: 20 | Loss: 2.23666 | test Loss: 2.23288\n",
      "Epoch: 30 | Loss: 2.07092 | test Loss: 2.06531\n",
      "Epoch: 40 | Loss: 1.82526 | test Loss: 1.81768\n",
      "Epoch: 50 | Loss: 1.67767 | test Loss: 1.68018\n",
      "Epoch: 60 | Loss: 1.61838 | test Loss: 1.62455\n",
      "Epoch: 70 | Loss: 1.57285 | test Loss: 1.58157\n",
      "Epoch: 80 | Loss: 1.54958 | test Loss: 1.56197\n",
      "Epoch: 90 | Loss: 1.53514 | test Loss: 1.55047\n",
      "Epoch: 100 | Loss: 1.52577 | test Loss: 1.54436\n",
      "Epoch: 110 | Loss: 1.51802 | test Loss: 1.54212\n",
      "Epoch: 120 | Loss: 1.51187 | test Loss: 1.54173\n",
      "Epoch: 130 | Loss: 1.50679 | test Loss: 1.54102\n",
      "Epoch: 140 | Loss: 1.50219 | test Loss: 1.54019\n",
      "Epoch: 150 | Loss: 1.49769 | test Loss: 1.54025\n",
      "Epoch: 160 | Loss: 1.49371 | test Loss: 1.54030\n",
      "Epoch: 170 | Loss: 1.49064 | test Loss: 1.53998\n",
      "Epoch: 180 | Loss: 1.48825 | test Loss: 1.53950\n",
      "Epoch: 190 | Loss: 1.48632 | test Loss: 1.53944\n",
      "Epoch: 200 | Loss: 1.48479 | test Loss: 1.53896\n",
      "Epoch: 210 | Loss: 1.48347 | test Loss: 1.53839\n",
      "Epoch: 220 | Loss: 1.48209 | test Loss: 1.53703\n",
      "Epoch: 230 | Loss: 1.48121 | test Loss: 1.53665\n",
      "Epoch: 240 | Loss: 1.48049 | test Loss: 1.53693\n",
      "Epoch: 250 | Loss: 1.47987 | test Loss: 1.53758\n",
      "Epoch: 260 | Loss: 1.47938 | test Loss: 1.53774\n",
      "Epoch: 270 | Loss: 1.47906 | test Loss: 1.53772\n",
      "Epoch: 280 | Loss: 1.47864 | test Loss: 1.53785\n",
      "Epoch: 290 | Loss: 1.47810 | test Loss: 1.53810\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    classifier.train() # Setting the model in training mode\n",
    "    y_prob, x_p, y_lin = classifier(X_train) #forward pass\n",
    "    \n",
    "    loss = loss_fcn(y_prob,\n",
    "                    y_train.float())  # Compute Loss\n",
    "    loss.backward() # compute backward\n",
    "    optimizer.step() # update parameters\n",
    "    optimizer.zero_grad() #zero grad optimizer\n",
    "    \n",
    "    ## Testing\n",
    "    classifier.eval() # Setting the model in evalaution mode\n",
    "    with torch.inference_mode():\n",
    "        y_prob_pred, x_trans, y_lin_pred = classifier(X_test) # Estimating th emodel output in test data\n",
    "    \n",
    "    test_loss = loss_fcn(y_prob_pred,\n",
    "                         y_test.float()) # Evaluating loss\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f} | test Loss: {test_loss:.5f}') # Printing the performance of the model as it is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.750 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "classifier.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs, _, _ = classifier(X_test)\n",
    "    for tensor, label in zip(outputs, y_test):\n",
    "        predicted = torch.argmax(tensor)\n",
    "        label = torch.argmax(label) \n",
    "        total += 1\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.3f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()  # Setting the model in evaluation mode\n",
    "with torch.inference_mode():\n",
    "    y_prob_1, x_trans_total_1, y_est_1 = classifier(\n",
    "        X_sample\n",
    "    )  # Computing the nonlinear transformation of the input data X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(63173) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3902d947d81c4e14a814ffc94ce68a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Number of tensors:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342f9540a80c442d9bd222b5524887cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761dbbc1f757402f8f1a74261a1971c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7507e5a655e4a5381d6653cffa105d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52be2f5a69d44f79e02833732f5a02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d480317be4d4ddb95f85f85c1b4ea96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0303470f00184d729486885f15831399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ed34c9413a4a269f847b1f69e3188e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8537c50e83475b8a98a839add3bbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1fe984b92447ed933336a8d60b8fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ea30bc5aad490c9d723ecd772e69b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lenght of tensors:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "betas_tensor = NObSP_NN_single_MultiOutput_reg(\n",
    "    X_sample, y_est_1, classifier\n",
    ")  # Computing the decomposition using NObSP. The Alpha parameters are the weigths for the Interpretation Layer\n",
    "torch.save(betas_tensor, \"betas_tensor_small.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nobsp_cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
